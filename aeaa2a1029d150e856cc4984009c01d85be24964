{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "9ace94d8_4ffa0008",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1003556
      },
      "writtenOn": "2025-11-26T14:12:30Z",
      "side": 1,
      "message": "This isn\u0027t stable if any client disconnects?\n\ne.g. all clients with [mover1, mover2, mover3]\n- your hash is 44%3\u003d2: would pick mover3\n- mover2 disconnects, hash is now 44%2\u003d0, it\u0027d pick mover1\n\nI guess that in practice this doesn\u0027t happen often so for something best effort it\u0027s probably good enough, but I\u0027m sure we can do better...\n\n\nThis feature feels *very* close to what the \"batch_archives\" options do, have you looked at that option?\nCould we discuss the design before the implementation? What\u0027s the goal here?\n\nIf you don\u0027t want to match the whole hint like the current code does, extracting some \"grouping\" sub-hint to decide on the batch would be fine (we just didn\u0027t need to do that in practice)\nConversely if you want the batches to reliably pick the same mover again when it frees up we can make the initial selection some sort of hash, but if there was no request for long enough for the batch to be forgotten I\u0027m not sure how useful that is either.\nAnyway, let\u0027s talk this out a bit before going into implementation details...",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c5506fcc_906d68f8",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1037727
      },
      "writtenOn": "2025-11-26T14:56:36Z",
      "side": 1,
      "message": "No, it\u0027s handled. I renamed the clients list to connected_clients and added a new clients list that contains all clients known to the coordinatool (connected or disconnected). So as long as a client is not free, even if disconnected, it will still be present in the list.",
      "parentUuid": "9ace94d8_4ffa0008",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4455a027_7e11e3f3",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1037727
      },
      "writtenOn": "2025-11-26T15:00:31Z",
      "side": 1,
      "message": "The “batch_archives” approach could work if we knew the grouping used in advance, but in our use case the grouping will depend on the files to be archived. So we can’t hard-code groupings to match in the configuration.\n\nWhat we want to do is always send requests with the same grouping to the same data mover, without knowing the grouping value beforehand. That’s why we decided to compute the hash and determine the data mover with `clients[hash(grouping) % nb_clients]`",
      "parentUuid": "c5506fcc_906d68f8",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f8e63196_588bbdb0",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1003556
      },
      "writtenOn": "2025-11-26T23:37:56Z",
      "side": 1,
      "message": "\u003e No, it\u0027s handled. I renamed the clients list to connected_clients and added a new clients list that contains all clients known to the coordinatool (connected or disconnected). So as long as a client is not free, even if disconnected, it will still be present in the list.\n\nI saw the new list, but I don\u0027t see why disconnected clients won\u0027t be freed after grace time.\nThere\u0027s also all the temporary clients (coordinatool-client commands) that show up in this list while they are active and get freed after a while, I wouldn\u0027t trust this code without extensive testing\n\n\u003e The “batch_archives” approach could work if we knew the grouping used in advance, but in our use case the grouping will depend on the files to be archived. So we can’t hard-code groupings to match in the configuration.\n\nFWIW, Stanford use for grouping is that they have a hint in the robinhood policy configuration and they run with a different policy based on filename, which will give a different hint and generate a different \"group\".\nThis feels identical so far.\n\n\u003e without knowing the grouping value beforehand.\n\nnote this isn\u0027t the \"archive_on_host\" feature with a hardcoded mapping, batching is dynamic based on hint value\n\n\u003e What we want to do is always send requests with the same grouping to the same data mover, without knowing the grouping value beforehand. That’s why we decided to compute the hash and determine the data mover with clients[hash(grouping) % nb_clients]\n\nCould you clarify this point a bit more; what\u0027s the use case to always pick the same mover? More precisely, what\u0027s the benefit to picking the same mover if the tag comes back after another n batches have been processed on that mover meanwhile?\n\nMy rationale to push back here is simple enough: we already have two mechanisms to steer the archives (that actually can be used together), and tests are already complex enough. Adding a third mechanism makes combinations jump up from 4 patterns to 16 and I can\u0027t test this all.\n\nI think we can make the batch mechanism work for what you need though, if the requirement to always pick the same node based on a hash we could add an extra check when looking for a free slot to be stable (... perhaps a similar hash mechanism), but I\u0027d like to understand first",
      "parentUuid": "4455a027_7e11e3f3",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b3c47070_5cec6e28",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2025-11-27T11:57:53Z",
      "side": 1,
      "message": "The issue with the current features for our use case (batches and archive_on_host) is that:\n- we can\u0027t know in advance the name of the grouping (archive_on_host is not usable);\n- we want the same grouping to go to the same copytool since we have different tape libraries on each copytools. We want files with the same grouping in the same library. AFAIK the batched requests only guarantee that the requests from a single batch will go to the same copytool. After a batch is finished or rescheduled due to the time slice expiration, there is no guarantee that the next batch for the same grouping will go to the same copytool. So this doesn\u0027t handle our use case;\n- we can have requests with the same grouping but other hints be different. So we would have to at least extract the grouping as you suggested.\n\nYou are right that disconnections and temporary clients make this code fail. We already knew this for disconnections but I didn\u0027t think about the temporary clients. One solution would be to keep the names of the copytools in the config and do something like this:\n\n```\ncopytool \u003d copytools_from_cfg[hash(grouping) % len(copytools_from_cfg)]\n```\n\nThe length of this list is independant of temporary clients and disconnections. Now this does not guarantee that if we add or remove copytools, we will have the same results. In our mind, this is more a first step to have a simple solution for this problem. This gives us the hability to start using Phobos in production for a few months and take time to figure out a proper solution in Phobos.\n\nThe question that remains is what do we do if the copytool is disconnected. We will have to discuss this internally. One solution would be to simply fail the archive since in our use case, in that situation, a restore would also fail. But that is very specific to our use case. Archives can be restarted by robinhood and applications are not waiting for them to finish.",
      "parentUuid": "f8e63196_588bbdb0",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "646aae29_e23e326a",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1003556
      },
      "writtenOn": "2025-11-27T13:32:16Z",
      "side": 1,
      "message": "\u003e we want the same grouping to go to the same copytool since we have different tape libraries on each copytools\n\nThanks for this explanation, this makes much more sense\n\n\u003e AFAIK the batched requests only guarantee that the requests from a single batch will go to the same copytool\n\nRight, as long as the batch is alive there is no problem (so if you have a timeout long enough in config that ensures that a whole archive batch finishes before the timeout then it might be acceptable), but if that timeout expires and more requests with the same hint comes in then a different host could be picked, so I agree it\u0027s not suitable as is\n\n\u003e we can have requests with the same grouping but other hints be different. So we would have to at least extract the grouping as you suggested.\n\nFWIW archive_on_host is meant to be used like that -- Stéphane provides hints like project\u003dfoo,tag\u003dn1 project\u003dbar,tag\u003dn2 etc, archive_on_host ensures \"tag\u003dn1\" archives are always on same host (cosmetic at this point for stanford, but used to ensure efficient transfers when tags are split by tag to keep minio parities separate) and batches ensures projects are grouped together (so put on the same tape thanks to grouped put on phobos side) -- from what you said you will have grouping\u003dsomething in the hints, how is that grouping value decided?\n\nOne question I had with your \"different tape libraries on each copytool\" is that you might want to be explicit about library allocations, e.g. raid1 with one copy on each library, but that\u0027s not the plan right now correct?\nI had two more worries:\n- this hash system assumes the hash will be equally balanced, I\u0027m not sure how dbj2 would turn out in practice -- in particular if the tag is user-controlled it could be a security problem as it\u0027s probably not hard to predict where a file would go, but I\u0027m not sure where the hint comes from in your case?\n- I agree with explicitly listing all the copytools in config for hash stability, but there\u0027s still a problem with adding new copytools at a maintenance or when some put_locate() support is implemented in phobos -- are the hints limited in time so that you could let finish all related transfers before changing config and not worry about continuing from the previous hint?\n\nWith that in mind I wonder if it wouldn\u0027t be better to make robinhood statically allocate movers with archive_on_host, but if these two points are OK I think a hash with a list hardcoded in config is fine.\n\n\n\u003e The question that remains is what do we do if the copytool is disconnected. We will have to discuss this internally. One solution would be to simply fail the archive since in our use case, in that situation, a restore would also fail. But that is very specific to our use case. Archives can be restarted by robinhood and applications are not waiting for them to finish.\n\nwith archive_on_host we simply make the request wait for the copytool to come back. We create a disconnected client and put the request|batch there, so it\u0027s on hold effectively forever (the disconnected client is replaced when the mover connects, or timesout and since there\u0027s still no match a new disconnected client is created again... That\u0027s another advantage of re-using batches here: you wouldn\u0027t need to check each request individually, only once per batch, so with a reasonable timeout the churn is minimal even with thousands of pending requests)\n\n\n\nSo my suggestion if you really can\u0027t use archive_on_host at this point would be to add a static list in config and extend the archive_on_hosts code to work with hashes -- the config item can be different to express the \"grouping\u003d...\" to hash and list of hosts, but the code should be in schedule_host_mapping in scheduler.c so it plays well with batches without much extra testing (and it will work whether you use batches or not, even if I\u0027d recommend to use them)",
      "parentUuid": "b3c47070_5cec6e28",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "726eae45_395ddeba",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2025-11-28T09:46:14Z",
      "side": 1,
      "message": "-OK for adding the config list of client to manage temporary clients.\n\n-For reusing the archive_on_host mechanism, we still have to rethink on it and to find the right way to do it.",
      "parentUuid": "646aae29_e23e326a",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9e2e9a32_04694917",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1003556
      },
      "writtenOn": "2025-11-28T21:49:32Z",
      "side": 1,
      "message": "Please don\u0027t think of the two separately, there should only be one setting.\n\nFor example, `archive_on_host_rr grouping\u003d host1 host2 host3` would build a list,\nand `schedule_host_mapping()` can check on it if it is set -- that function is already called exactly when you want (when archive requests are processed, with or without batching).",
      "parentUuid": "726eae45_395ddeba",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c7c5c4b7_36ad89e3",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1037727
      },
      "writtenOn": "2025-12-01T08:51:52Z",
      "side": 1,
      "message": "We’d reached roughly the same conclusion. So the idea would be to extend the `archive_on_hosts` parameter into something like `archive_on_hosts_ch` (ch for consistent hashing). In our case, the configuration would contain be `archive_on_hosts_ch grouping\u003d host1 host2`.\n\nInternally, we would add a boolean in the `host_mapping` structure to indicate whether it’s the extended version with consistent hashing or not. And in `schedule_host_mapping()`, if this boolean is true, we select the client based on the hash of the tag value (the grouping for us, but it could be anything in the end). Otherwise, we keep the current behavior.\n\nWould that work for you?",
      "parentUuid": "9e2e9a32_04694917",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    }
  ]
}