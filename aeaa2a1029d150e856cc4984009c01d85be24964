{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "9ace94d8_4ffa0008",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1003556
      },
      "writtenOn": "2025-11-26T14:12:30Z",
      "side": 1,
      "message": "This isn\u0027t stable if any client disconnects?\n\ne.g. all clients with [mover1, mover2, mover3]\n- your hash is 44%3\u003d2: would pick mover3\n- mover2 disconnects, hash is now 44%2\u003d0, it\u0027d pick mover1\n\nI guess that in practice this doesn\u0027t happen often so for something best effort it\u0027s probably good enough, but I\u0027m sure we can do better...\n\n\nThis feature feels *very* close to what the \"batch_archives\" options do, have you looked at that option?\nCould we discuss the design before the implementation? What\u0027s the goal here?\n\nIf you don\u0027t want to match the whole hint like the current code does, extracting some \"grouping\" sub-hint to decide on the batch would be fine (we just didn\u0027t need to do that in practice)\nConversely if you want the batches to reliably pick the same mover again when it frees up we can make the initial selection some sort of hash, but if there was no request for long enough for the batch to be forgotten I\u0027m not sure how useful that is either.\nAnyway, let\u0027s talk this out a bit before going into implementation details...",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c5506fcc_906d68f8",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1037727
      },
      "writtenOn": "2025-11-26T14:56:36Z",
      "side": 1,
      "message": "No, it\u0027s handled. I renamed the clients list to connected_clients and added a new clients list that contains all clients known to the coordinatool (connected or disconnected). So as long as a client is not free, even if disconnected, it will still be present in the list.",
      "parentUuid": "9ace94d8_4ffa0008",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4455a027_7e11e3f3",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1037727
      },
      "writtenOn": "2025-11-26T15:00:31Z",
      "side": 1,
      "message": "The “batch_archives” approach could work if we knew the grouping used in advance, but in our use case the grouping will depend on the files to be archived. So we can’t hard-code groupings to match in the configuration.\n\nWhat we want to do is always send requests with the same grouping to the same data mover, without knowing the grouping value beforehand. That’s why we decided to compute the hash and determine the data mover with `clients[hash(grouping) % nb_clients]`",
      "parentUuid": "c5506fcc_906d68f8",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f8e63196_588bbdb0",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1003556
      },
      "writtenOn": "2025-11-26T23:37:56Z",
      "side": 1,
      "message": "\u003e No, it\u0027s handled. I renamed the clients list to connected_clients and added a new clients list that contains all clients known to the coordinatool (connected or disconnected). So as long as a client is not free, even if disconnected, it will still be present in the list.\n\nI saw the new list, but I don\u0027t see why disconnected clients won\u0027t be freed after grace time.\nThere\u0027s also all the temporary clients (coordinatool-client commands) that show up in this list while they are active and get freed after a while, I wouldn\u0027t trust this code without extensive testing\n\n\u003e The “batch_archives” approach could work if we knew the grouping used in advance, but in our use case the grouping will depend on the files to be archived. So we can’t hard-code groupings to match in the configuration.\n\nFWIW, Stanford use for grouping is that they have a hint in the robinhood policy configuration and they run with a different policy based on filename, which will give a different hint and generate a different \"group\".\nThis feels identical so far.\n\n\u003e without knowing the grouping value beforehand.\n\nnote this isn\u0027t the \"archive_on_host\" feature with a hardcoded mapping, batching is dynamic based on hint value\n\n\u003e What we want to do is always send requests with the same grouping to the same data mover, without knowing the grouping value beforehand. That’s why we decided to compute the hash and determine the data mover with clients[hash(grouping) % nb_clients]\n\nCould you clarify this point a bit more; what\u0027s the use case to always pick the same mover? More precisely, what\u0027s the benefit to picking the same mover if the tag comes back after another n batches have been processed on that mover meanwhile?\n\nMy rationale to push back here is simple enough: we already have two mechanisms to steer the archives (that actually can be used together), and tests are already complex enough. Adding a third mechanism makes combinations jump up from 4 patterns to 16 and I can\u0027t test this all.\n\nI think we can make the batch mechanism work for what you need though, if the requirement to always pick the same node based on a hash we could add an extra check when looking for a free slot to be stable (... perhaps a similar hash mechanism), but I\u0027d like to understand first",
      "parentUuid": "4455a027_7e11e3f3",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b3c47070_5cec6e28",
        "filename": "copytool/phobos.c",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2025-11-27T11:57:53Z",
      "side": 1,
      "message": "The issue with the current features for our use case (batches and archive_on_host) is that:\n- we can\u0027t know in advance the name of the grouping (archive_on_host is not usable);\n- we want the same grouping to go to the same copytool since we have different tape libraries on each copytools. We want files with the same grouping in the same library. AFAIK the batched requests only guarantee that the requests from a single batch will go to the same copytool. After a batch is finished or rescheduled due to the time slice expiration, there is no guarantee that the next batch for the same grouping will go to the same copytool. So this doesn\u0027t handle our use case;\n- we can have requests with the same grouping but other hints be different. So we would have to at least extract the grouping as you suggested.\n\nYou are right that disconnections and temporary clients make this code fail. We already knew this for disconnections but I didn\u0027t think about the temporary clients. One solution would be to keep the names of the copytools in the config and do something like this:\n\n```\ncopytool \u003d copytools_from_cfg[hash(grouping) % len(copytools_from_cfg)]\n```\n\nThe length of this list is independant of temporary clients and disconnections. Now this does not guarantee that if we add or remove copytools, we will have the same results. In our mind, this is more a first step to have a simple solution for this problem. This gives us the hability to start using Phobos in production for a few months and take time to figure out a proper solution in Phobos.\n\nThe question that remains is what do we do if the copytool is disconnected. We will have to discuss this internally. One solution would be to simply fail the archive since in our use case, in that situation, a restore would also fail. But that is very specific to our use case. Archives can be restarted by robinhood and applications are not waiting for them to finish.",
      "parentUuid": "f8e63196_588bbdb0",
      "revId": "aeaa2a1029d150e856cc4984009c01d85be24964",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    }
  ]
}